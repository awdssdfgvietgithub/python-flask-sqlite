{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4891,
     "status": "ok",
     "timestamp": 1734607768664,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "PVi9rEosQUGm",
    "outputId": "73b0b2c9-b3ee-4102-b742-195dd7cf9fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting node2vec\n",
      "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.3)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.4.2)\n",
      "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.67.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.17.0)\n",
      "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
      "Installing collected packages: node2vec\n",
      "Successfully installed node2vec-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHoED-8mANeq"
   },
   "source": [
    "# PHẦN 1 - CHUẨN BỊ DỮ LIỆU TRAIN DATA VÀ TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7260,
     "status": "ok",
     "timestamp": 1734584290422,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "7K4ZbmIl_V_0",
    "outputId": "9d6b7882-2e9b-4c70-d0f1-fa2df8c89917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu chạy code Chuẩn bị dữ liệu\n",
      "Tiến hành tách cạnh thành tập huấn luyện và kiểm tra....\n",
      "Tạo tập dữ liệu huấn luyện và kiểm tra của bộ positive edges\n",
      "Số positive edges (train/test): 70587/17647\n",
      "Tạo tập dữ liệu huấn luyện và kiểm tra của bộ negative edges\n",
      "Số negative edges (train/test): 70587/17647\n",
      "Trai-Test checkpoint saved successfully.\n",
      "Đã hoàn thành việc tạo dữ liệu train-test - Hết code\n"
     ]
    }
   ],
   "source": [
    "# Phần 1: Chuẩn bị dữ liệu\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "is_checkpoint_loaded = True\n",
    "\n",
    "print(\"Bắt đầu chạy code Chuẩn bị dữ liệu\")\n",
    "try:\n",
    "  project_path =\"/content/drive/MyDrive/Chương trình thạc sĩ/Mạng xã hội - Social Network/Đồ án môn học/\"\n",
    "\n",
    "  # Đường dẫn file lưu dữ liệu huấn luyện\n",
    "  train_test_data_checkpoint_path = f\"{project_path}train_test_data.pkl\"\n",
    "\n",
    "  # Load đồ thị từ file graphml\n",
    "  G = nx.read_graphml(f\"{project_path}processed_graph.graphml\")\n",
    "\n",
    "  import os\n",
    "  # Kiểm tra nếu checkpoint tồn tại\n",
    "  if os.path.exists(train_test_data_checkpoint_path) and is_checkpoint_loaded:\n",
    "    # Tải lại checkpoint\n",
    "    with open(train_test_data_checkpoint_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data = data[\"train_data\"]\n",
    "    train_labels = data[\"train_labels\"]\n",
    "    test_data = data[\"test_data\"]\n",
    "    test_labels = data[\"test_labels\"]\n",
    "    print(\"Checkpoint loaded successfully.\")\n",
    "  else:\n",
    "    # Tách cạnh thành tập huấn luyện và kiểm tra\n",
    "    print (\"Tiến hành tách cạnh thành tập huấn luyện và kiểm tra....\")\n",
    "\n",
    "    # Lấy danh sách tất cả các cạnh có kết nối\n",
    "    positive_edges = list(G.edges())\n",
    "\n",
    "    # Tách positive_train và positive_test theo tỷ lệ 80 - 20\n",
    "    print (\"Tạo tập dữ liệu huấn luyện và kiểm tra của bộ positive edges\")\n",
    "    positive_train, positive_test = train_test_split(positive_edges, test_size=0.2, random_state=42)\n",
    "    print(f\"Số positive edges (train/test): {len(positive_train)}/{len(positive_test)}\")\n",
    "\n",
    "\n",
    "    # 3. Tạo negative edges (các cạnh không liên kết)\n",
    "\n",
    "    # Số lượng negative-edges cần lấy mẫu\n",
    "    num_samples = len(positive_edges)  # Số lượng bằng positive_edges\n",
    "    print(\"Tạo tập dữ liệu huấn luyện và kiểm tra của bộ negative edges\")\n",
    "    negative_edges = list(islice(nx.non_edges(G), num_samples))\n",
    "\n",
    "    # Tách negative_train và negative_test theo tỷ lệ 80 - 20\n",
    "    negative_train, negative_test = train_test_split(negative_edges, test_size=0.2, random_state=42)\n",
    "    print(f\"Số negative edges (train/test): {len(negative_train)}/{len(negative_test)}\")\n",
    "\n",
    "    # Tạo tập dữ liệu huấn luyện và tập dữ liệu kiểm tra\n",
    "    train_data = positive_train + negative_train\n",
    "    train_labels = [1] * len(positive_train) + [0] * len(negative_train)\n",
    "    test_data = positive_test + negative_test\n",
    "    test_labels = [1] * len(positive_test) + [0] * len(negative_test)\n",
    "\n",
    "    data = {\n",
    "        \"train_data\": train_data,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"test_data\": test_data,\n",
    "        \"test_labels\": test_labels\n",
    "    }\n",
    "    # Lưu checkpoint\n",
    "    with open(train_test_data_checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"Train-Test checkpoint saved successfully.\")\n",
    "\n",
    "  print(\"Đã hoàn thành việc tạo dữ liệu train-test - Hết code\")\n",
    "except Exception as e:\n",
    "  print(f\"Lỗi xảy ra: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F-penGkyI8q"
   },
   "source": [
    "# Phần 2: Node2Vec - Tạo vector nhúng và bổ sung đặc trưng\n",
    "\n",
    "Tạo vector nhúng từ bộ dữ liệu train-test (Node2Vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414,
     "referenced_widgets": [
      "9953e264b12046479b43b8e3269be48d",
      "62866b75178a4fde848a3ed7833dad40",
      "32ba96ddee12409783431c2ecc0b44e1",
      "664fa514bb1543a398e3a95380773ed6",
      "360dc743c11447c88fccbb159d26a634",
      "1ec1525d2f174217a4fe503fedfdc815",
      "ed9e8777566843eb8186aded8ffbd609",
      "9837c2b906ad41c4aff8b111d9cdfa21",
      "33592f9cb1654d9ea83b33c2079c423b",
      "4a178cc36a51456f89a6d7bf79767196",
      "181e7b942f2245d2b52dcb9a51cd46c8"
     ]
    },
    "executionInfo": {
     "elapsed": 933615,
     "status": "ok",
     "timestamp": 1734601745774,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "gi9nBZHNTCcq",
    "outputId": "cf890895-f102-43f6-da2f-64f499390929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiểm tra và Đảm bảo load dữ liệu từ phần 1\n",
      "Train and test data loaded successfully from checkpoints.\n",
      "Huấn luyện Node2Vec để tạo vector nhúng...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9953e264b12046479b43b8e3269be48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/4039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu huấn luyện Word2Vec...\n",
      "Epoch 1 started...\n",
      "Epoch 1 finished.\n",
      "Epoch 2 started...\n",
      "Epoch 2 finished.\n",
      "Epoch 3 started...\n",
      "Epoch 3 finished.\n",
      "Epoch 4 started...\n",
      "Epoch 4 finished.\n",
      "Epoch 5 started...\n",
      "Epoch 5 finished.\n",
      "Huấn luyện Node2Vec hoàn thành!\n",
      "Saving Node2Vec embeddings to checkpoint...\n",
      "Node2Vec embeddings saved successfully.\n",
      "Saving full Node2Vec model to checkpoint...\n",
      "Full Node2Vec model saved successfully.\n",
      "Đã hoàn thành tạo vector nhúng  - Hết code\n"
     ]
    }
   ],
   "source": [
    "#--- Phần 2: Tạo vector nhúng\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import pickle\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# Đường dẫn đến thư mục dự án\n",
    "project_path =\"/content/drive/MyDrive/Chương trình thạc sĩ/Mạng xã hội - Social Network/Đồ án môn học/\"\n",
    "# Đường dẫn đến file lưu Train-Test check point\n",
    "train_test_checkpoint_path = f\"{project_path}train_test_data.pkl\"\n",
    "# Đường dẫn file trainmodel và testmodel checkpoint\n",
    "trainmodel_checkpoint_path = f\"{project_path}train_model_data.pkl\"\n",
    "testmodel_checkpoint_path = f\"{project_path}test_model_data.pkl\"\n",
    "\n",
    "node2vec_checkpoint_path = f\"{project_path}node2vec_embeddings.kv\"\n",
    "node2vec_full_model_checkpoint_path = f\"{project_path}node2vec_full_model.pkl\"\n",
    "\n",
    "is_checkpoint_loaded = True # Biến xác định có muốn thực hiện load check point hay\n",
    "\n",
    "# Load đồ thị từ file graphml\n",
    "G = nx.read_graphml(f\"{project_path}processed_graph.graphml\")\n",
    "\n",
    "# Callback để theo dõi tiến trình Word2Vec\n",
    "class ProgressCallback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch {self.epoch + 1} started...\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(f\"Epoch {self.epoch + 1} finished.\")\n",
    "        self.epoch += 1\n",
    "\n",
    "try:\n",
    "    # Kiểm tra và Đảm bảo load dữ liệu từ phần 1\n",
    "    print(\"Kiểm tra và Đảm bảo load dữ liệu từ phần 1\")\n",
    "    if os.path.exists(train_test_checkpoint_path):\n",
    "        with open(train_test_checkpoint_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        train_data= data[\"train_data\"]\n",
    "        train_labels= data[\"train_labels\"]\n",
    "        test_data= data[\"test_data\"]\n",
    "        test_labels= data[\"test_labels\"]\n",
    "        print(\"Train and test data loaded successfully from checkpoints.\")\n",
    "    else:\n",
    "        # Không tìm thấy Train và Test checkpoint\n",
    "        print(\"Error: Train and test data not found. Please run Part 1 to generate them.\")\n",
    "        raise Exception(\"Train and test data not found in checkpoints.\")\n",
    "\n",
    "    # Huấn luyện hoặc tải lại Node2Vec để tạo vector nhúng\n",
    "    if os.path.exists(node2vec_full_model_checkpoint_path) and is_checkpoint_loaded:\n",
    "        # Tải lại checkpoint của toàn bộ mô hình đã được lưu\n",
    "        print(\"Loading full Node2Vec model from checkpoint...\")\n",
    "        with open(node2vec_full_model_checkpoint_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(\"Full Node2Vec model loaded successfully.\")\n",
    "    elif os.path.exists(node2vec_checkpoint_path) and is_checkpoint_loaded:\n",
    "        # Tải lại checkpoint của vector nhúng đã được lưu\n",
    "        print(\"Loading Node2Vec embeddings from checkpoint...\")\n",
    "        # Khởi tạo Node2Vec\n",
    "        model = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "        # Load Node2Vec embeddings from checkpoint\n",
    "        model.wv = KeyedVectors.load(node2vec_checkpoint_path)\n",
    "        print(\"Node2Vec embeddings loaded successfully.\")\n",
    "    else:\n",
    "        #Huấn luyện Node2Vec để tạo vector nhúng\n",
    "        print(\"Huấn luyện Node2Vec để tạo vector nhúng...\")\n",
    "        # Khởi tạo Node2Vec\n",
    "        node2vec = Node2Vec(G, dimensions=64, walk_length=20, num_walks=100, workers=4)\n",
    "\n",
    "        # Huấn luyện Word2Vec với callback\n",
    "        print(\"Bắt đầu huấn luyện Word2Vec...\")\n",
    "        model = node2vec.fit(window=10, min_count=1, batch_words=4, callbacks=[ProgressCallback()])\n",
    "        print(\"Huấn luyện Node2Vec hoàn thành!\")\n",
    "\n",
    "        # Lưu Node2Vec embeddings\n",
    "        print(\"Saving Node2Vec embeddings to checkpoint...\")\n",
    "        model.wv.save(node2vec_checkpoint_path)\n",
    "        print(\"Node2Vec embeddings saved successfully.\")\n",
    "\n",
    "        # Lưu toàn bộ mô hình Node2Vec embeddings\n",
    "        print(\"Saving full Node2Vec model to checkpoint...\")\n",
    "        with open(node2vec_full_model_checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(\"Full Node2Vec model saved successfully.\")\n",
    "\n",
    "\n",
    "    print(\"Đã hoàn thành tạo vector nhúng  - Hết code\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi xảy ra: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY3a5OPkAvSB"
   },
   "source": [
    "# PHẦN 3: TẠO ĐẶC TRƯNG CHO CÁC NODE\n",
    "Tăng cường đặc trưng của node bằng:\n",
    "  - Tính Degree và Clustering Coefficient\n",
    "  - Thêm đặc tính Jaccard Coefficient và Shortest Path Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257769,
     "status": "ok",
     "timestamp": 1734602505026,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "k6BHO6-oPuR6",
    "outputId": "ca33c48a-b5fb-4b3e-dc6d-262737d43f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu chạy code TẠO ĐẶC TRƯNG CHO CÁC NODE\n",
      "Bắt đầu bổ sung đặc trưng cho các cặp node...\n",
      "Tính Jaccard Coefficient...\n",
      "Tính Shortest Path Distance...\n",
      "Kiểm tra và Đảm bảo load dữ liệu từ phần 1\n",
      "Train and test data loaded successfully from checkpoints.\n",
      "Bổ sung đặc trưng cho train_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 141174/141174 [00:03<00:00, 35503.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bổ sung đặc trưng cho test_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 35294/35294 [00:02<00:00, 17095.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lưu checkpoint cho train_features và test_features...\n",
      "Train features shape: (141174, 7)\n",
      "Test features shape: (35294, 7)\n",
      "Feature extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "is_checkpoint_loaded = True\n",
    "print(\"Bắt đầu chạy code TẠO ĐẶC TRƯNG CHO CÁC NODE\")\n",
    "try:\n",
    "    project_path =\"/content/drive/MyDrive/Chương trình thạc sĩ/Mạng xã hội - Social Network/Đồ án môn học/\"\n",
    "    # Load đồ thị từ file graphml\n",
    "    G = nx.read_graphml(f\"{project_path}processed_graph.graphml\")\n",
    "\n",
    "    print(\"Bắt đầu bổ sung đặc trưng cho các cặp node...\")\n",
    "    # Dictionary lưu các đặc trưng\n",
    "    feature_dict = {}\n",
    "    # Tính Jaccard Coefficient\n",
    "    print(\"Tính Jaccard Coefficient...\")\n",
    "    jaccard_coeffs = {\n",
    "        tuple(sorted((u, v))): coeff for u, v, coeff in nx.jaccard_coefficient(G)\n",
    "        }\n",
    "\n",
    "    # Tính Shortest Path Distance\n",
    "    print(\"Tính Shortest Path Distance...\")\n",
    "    shortest_paths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    def compute_shortest_path(u, v, shortest_paths):\n",
    "      try:\n",
    "          return shortest_paths[u][v]\n",
    "      except KeyError:\n",
    "          return np.inf  # Không có đường đi giữa u và v\n",
    "      except Exception as e:\n",
    "          print(f\"Error computing shortest path for ({u}, {v}): {e}\")\n",
    "          return np.inf\n",
    "\n",
    "    # Bổ sung đặc trưng cho train_data và test_data\n",
    "    def extract_features(data, feature_dict):\n",
    "      features = []\n",
    "      clustering_cache = nx.clustering(G)  # Tính trước hệ số cụm cho tất cả các nút\n",
    "      degrees_cache = dict(G.degree())    # Tính trước bậc của tất cả các nút\n",
    "      for u, v in tqdm(data, desc=\"Extracting features\"):\n",
    "        common_neighbors = len(list(nx.common_neighbors(G, u, v)))\n",
    "        degree_u = degrees_cache.get(u, 0)\n",
    "        degree_v = degrees_cache.get(v, 0)\n",
    "        clustering_u = clustering_cache.get(u, 0)\n",
    "        clustering_v = clustering_cache.get(v, 0)\n",
    "        jaccard_coeff = jaccard_coeffs.get(tuple(sorted((u, v))), 0)\n",
    "        shortest_path = compute_shortest_path(u, v, shortest_paths)\n",
    "\n",
    "        feature_dict[(u, v)] = [\n",
    "            common_neighbors,\n",
    "            degree_u,\n",
    "            degree_v,\n",
    "            clustering_u,\n",
    "            clustering_v,\n",
    "            jaccard_coeff,\n",
    "            shortest_path,\n",
    "        ]\n",
    "        features.append(feature_dict[(u, v)])\n",
    "      return features\n",
    "\n",
    "    # load dữ liệu Train_data và Test_data từ phần 1\n",
    "    # Đường dẫn file lưu dữ liệu huấn luyện\n",
    "    train_test_data_checkpoint_path = f\"{project_path}train_test_data.pkl\"\n",
    "    print(\"Kiểm tra và Đảm bảo load dữ liệu từ phần 1\")\n",
    "    if os.path.exists(train_test_checkpoint_path):\n",
    "        with open(train_test_checkpoint_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        train_data= data[\"train_data\"]\n",
    "        test_data= data[\"test_data\"]\n",
    "        print(\"Train and test data loaded successfully from checkpoints.\")\n",
    "    else:\n",
    "        # Không tìm thấy Train và Test checkpoint\n",
    "        print(\"Error: Train and test data not found. Please run Part 1 to generate them.\")\n",
    "        raise Exception(\"Train and test data not found in checkpoints.\")\n",
    "\n",
    "    # Tính đặc trưng cho train_data\n",
    "    print(\"Bổ sung đặc trưng cho train_data...\")\n",
    "    train_features = extract_features(train_data, feature_dict)\n",
    "\n",
    "    # Tính đặc trưng cho test_data\n",
    "    print(\"Bổ sung đặc trưng cho test_data...\")\n",
    "    test_features = extract_features(test_data, feature_dict)\n",
    "\n",
    "    # Chuyển đổi thành DataFrame để dễ sử dụng\n",
    "    columns = [\n",
    "        \"common_neighbors\",\n",
    "        \"degree_u\",\n",
    "        \"degree_v\",\n",
    "        \"clustering_u\",\n",
    "        \"clustering_v\",\n",
    "        \"jaccard_coeff\",\n",
    "        \"shortest_path\",\n",
    "    ]\n",
    "    train_features_df = pd.DataFrame(train_features, columns=columns)\n",
    "    test_features_df = pd.DataFrame(test_features, columns=columns)\n",
    "\n",
    "    # Lưu checkpoint của các đặc trưng\n",
    "    nodes_features_checkpoint_path = f\"{project_path}nodes_features.pkl\"\n",
    "\n",
    "\n",
    "    print(\"Lưu checkpoint cho train_features và test_features...\")\n",
    "    with open(nodes_features_checkpoint_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"train_features\": train_features_df,\n",
    "            \"test_features\": test_features_df\n",
    "        }, f)\n",
    "\n",
    "    print(f\"Train features shape: {train_features_df.shape}\")\n",
    "    print(f\"Test features shape: {test_features_df.shape}\")\n",
    "    print(\"Feature extraction completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Lỗi xảy ra: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07geV03KHFJC"
   },
   "source": [
    "# PHẦN 4: GỘP ĐẶC TRƯNG CỦA CÁC NODE VỚI VECTOR ĐẶC TRƯNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11898,
     "status": "ok",
     "timestamp": 1734603455390,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "NRcKD92PHNfr",
    "outputId": "2ed635ea-e352-4fc2-b267-7f541cabb839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu chạy code GỘP ĐẶC TRƯNG CÁC NODE VỚI VECTOR ĐẶC TRƯNG\n",
      "Kiểm tra và Đảm bảo load dữ liệu từ phần 1\n",
      "Train and test data loaded successfully from checkpoints.\n",
      "Kiểm tra và Đảm bảo load dữ liệu đặc trưng Train_data và Test_data từ phần 3\n",
      "Nodes Feature loaded successfully from checkpoints.\n",
      "Kiểm tra và Đảm bảo load Node2Vec model từ phần 2\n",
      "Node2Vec model loaded successfully from checkpoints.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining features: 100%|██████████| 141174/141174 [00:05<00:00, 28082.53it/s]\n",
      "Combining features: 100%|██████████| 35294/35294 [00:01<00:00, 31189.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lưu checkpoint cho train_combined_features và test_combined_features...\n",
      "Train combined features shape: 141174 x 135\n",
      "Test combined features shape: 35294 x 135\n",
      "Đã hoàn thành kết hợp vector nhúng và các đặc trưng!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "is_checkpoint_loaded = True\n",
    "project_path =\"/content/drive/MyDrive/Chương trình thạc sĩ/Mạng xã hội - Social Network/Đồ án môn học/\"\n",
    "\n",
    "print(\"Bắt đầu chạy code GỘP ĐẶC TRƯNG CÁC NODE VỚI VECTOR ĐẶC TRƯNG\")\n",
    "try:\n",
    "    # Load dữ liệu Train_data và Test_data từ phần 1\n",
    "    # Đường dẫn file lưu dữ liệu huấn luyện\n",
    "    train_test_data_checkpoint_path = f\"{project_path}train_test_data.pkl\"\n",
    "    print(\"Kiểm tra và Đảm bảo load dữ liệu từ phần 1\")\n",
    "    if os.path.exists(train_test_data_checkpoint_path):\n",
    "        with open(train_test_data_checkpoint_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        train_data= data[\"train_data\"]\n",
    "        test_data= data[\"test_data\"]\n",
    "        print(\"Train and test data loaded successfully from checkpoints.\")\n",
    "    else:\n",
    "        # Không tìm thấy Train và Test checkpoint\n",
    "        print(\"Error: Train and test data not found. Please run Part 1 to generate them.\")\n",
    "        raise Exception(\"Train and test data not found in checkpoints.\")\n",
    "\n",
    "    # Load dữ liệu đặc trưng Train_data và Test_data từ phần 3\n",
    "    nodes_features_checkpoint_path = f\"{project_path}nodes_features.pkl\"\n",
    "    print(\"Kiểm tra và Đảm bảo load dữ liệu đặc trưng Train_data và Test_data từ phần 3\")\n",
    "    # Load dữ liệu Train features\n",
    "    if os.path.exists(nodes_features_checkpoint_path):\n",
    "        with open(nodes_features_checkpoint_path, 'rb') as f:\n",
    "            features_data = pickle.load(f)\n",
    "        train_features = features_data[\"train_features\"]\n",
    "        test_features = features_data[\"test_features\"]\n",
    "        print(\"Nodes Feature loaded successfully from checkpoints.\")\n",
    "    else:\n",
    "        # Không tìm thấy Nodes Features checkpoint\n",
    "        print(\"Error: Nodes Feature data not found. Please run Part 3 to generate them.\")\n",
    "        raise Exception(\"Nodes Feature data not found in checkpoints.\")\n",
    "\n",
    "    # Load Node2Vec model from Part 2\n",
    "    node2vec_full_model_checkpoint_path = f\"{project_path}node2vec_full_model.pkl\"\n",
    "    print(\"Kiểm tra và Đảm bảo load Node2Vec model từ phần 2\")\n",
    "    if os.path.exists(node2vec_full_model_checkpoint_path):\n",
    "        with open(node2vec_full_model_checkpoint_path, 'rb') as f:\n",
    "            node2vec_model = pickle.load(f)\n",
    "        print(\"Node2Vec model loaded successfully from checkpoints.\")\n",
    "    else:\n",
    "        # Không tìm thấy Train và Test checkpoint\n",
    "        print(\"Error: Node2Vec model data not found. Please run Part 2 to generate them.\")\n",
    "        raise Exception(\"Node2Vec model data not found in checkpoints.\")\n",
    "\n",
    "    # Hàm gộp Node2Vec model với đặc trưng các node\n",
    "    def combine_features(data, feature_dict, model):\n",
    "        combined_features = []\n",
    "        for u, v in tqdm(data, desc=\"Combining features\"):\n",
    "            # Vector nhúng của u và v\n",
    "            embedding_u = model.wv[u] if u in model.wv else np.zeros(model.vector_size)\n",
    "            embedding_v = model.wv[v] if v in model.wv else np.zeros(model.vector_size)\n",
    "\n",
    "             # Đặc trưng thủ công\n",
    "            handcrafted_features = feature_dict.get((u, v), np.zeros(7))  # 7: số lượng đặc trưng (đã kiểm tra)\n",
    "\n",
    "            # Kết hợp vector nhúng và đặc trưng thủ công\n",
    "            combined_features.append(\n",
    "                np.concatenate([embedding_u, embedding_v, handcrafted_features])\n",
    "            )\n",
    "        return combined_features\n",
    "\n",
    "    # Kết hợp cho train_data\n",
    "    train_combined_features = combine_features(train_data, train_features, node2vec_model)\n",
    "\n",
    "    # Kết hợp cho test_data\n",
    "    test_combined_features = combine_features(test_data, test_features, node2vec_model)\n",
    "\n",
    "    # Lưu checkpoint\n",
    "    train_combined_checkpoint_path = f\"{project_path}train_combined_features.pkl\"\n",
    "    test_combined_checkpoint_path = f\"{project_path}test_combined_features.pkl\"\n",
    "\n",
    "    print(\"Lưu checkpoint cho train_combined_features và test_combined_features...\")\n",
    "    with open(train_combined_checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(train_combined_features, f)\n",
    "    with open(test_combined_checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(test_combined_features, f)\n",
    "\n",
    "     # Hiển thị thông tin kết quả\n",
    "    print(f\"Train combined features shape: {len(train_combined_features)} x {len(train_combined_features[0])}\")\n",
    "    print(f\"Test combined features shape: {len(test_combined_features)} x {len(test_combined_features[0])}\")\n",
    "    print(\"Đã hoàn thành kết hợp vector nhúng và các đặc trưng!\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Lỗi xảy ra: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9ifIj5QbIEg"
   },
   "source": [
    "# PHẦN 5.1: MACHINE LEARNING - THUẬT TOÁN RANDOM FOREST\n",
    "- Dùng thuật toán RANDOM FOREST\n",
    "- Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351317,
     "status": "ok",
     "timestamp": 1734613136347,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "1ok6VlD8beQ_",
    "outputId": "62b2ee5d-f462-4532-8f88-51d7fb2430b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiểm tra và Đảm bảo load dữ liệu từ phần 1\n",
      "Train-Test Data loaded successfully.\n",
      "Train and test data loaded successfully from checkpoints.\n",
      "Kiểm tra và Đọc lại checkpoint đặc trưng của Train và Test Data từ Phần 4\n",
      "Train Combined Features loaded successfully.\n",
      "Test Combined Features loaded successfully.\n",
      "Combined features loaded successfully from checkpoints.\n",
      "Huấn luyện mô hình Random Forest\n",
      "Model trained successfully.\n",
      "Model saved successfully.\n",
      "KẾT QUẢ ĐÁNH GIÁ MÔ HÌNH RANDOM FOREST\n",
      "Train Accuracy: 0.9970178644792951\n",
      "Test Accuracy: 0.9970249900833003\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     17647\n",
      "           1       1.00      0.99      1.00     17647\n",
      "\n",
      "    accuracy                           1.00     35294\n",
      "   macro avg       1.00      1.00      1.00     35294\n",
      "weighted avg       1.00      1.00      1.00     35294\n",
      "\n",
      "AUC: 0.9994791077440575\n",
      "F1 Score: 0.9970166216792158\n",
      "Precision: 0.9998290403464782\n",
      "Recall: 0.9942199807332691\n",
      "Cross-validation scores: [0.99730831 0.99727289 0.99663538 0.99635205 0.9974853 ]\n",
      "Mean CV accuracy: 0.9970107843832222\n"
     ]
    }
   ],
   "source": [
    "#Huấn luyện và đánh giá mô hình\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "project_path =\"/content/drive/MyDrive/Chương trình thạc sĩ/Mạng xã hội - Social Network/Đồ án môn học/\"\n",
    "model_checkpoint = f\"{project_path}ML_RandomForest_model.pkl\"\n",
    "is_checkpoint_loaded = True\n",
    "\n",
    "# Hàm load check point\n",
    "def load_checkpoint(path, description=\"Checkpoint\"):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            print(f\"{description} loaded successfully.\")\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{description} not found at {path}.\")\n",
    "\n",
    "try:\n",
    "  # Đường dẫn checkpoint các file gộp đặc trưng của Train và Test Data từ Phần 4\n",
    "  train_combined_checkpoint_path = f\"{project_path}train_combined_features.pkl\"\n",
    "  test_combined_checkpoint_path = f\"{project_path}test_combined_features.pkl\"\n",
    "\n",
    "  # Load dữ liệu Train_data và Test_data từ phần 1\n",
    "  # Đường dẫn file lưu dữ liệu huấn luyện\n",
    "  train_test_data_checkpoint_path = f\"{project_path}train_test_data.pkl\"\n",
    "  print(\"Kiểm tra và Đảm bảo load dữ liệu từ phần 1\")\n",
    "  train_test_data = load_checkpoint(f\"{project_path}train_test_data.pkl\", \"Train-Test Data\")\n",
    "  train_labels, test_labels = train_test_data[\"train_labels\"], train_test_data[\"test_labels\"]\n",
    "  print(\"Train and test data loaded successfully from checkpoints.\")\n",
    "\n",
    "  # Kiểm tra và Đọc lại checkpoint từ Phần 4\n",
    "  print(\"Kiểm tra và Đọc lại checkpoint đặc trưng của Train và Test Data từ Phần 4\")\n",
    "  X_train = load_checkpoint(f\"{project_path}train_combined_features.pkl\", \"Train Combined Features\")\n",
    "  X_test = load_checkpoint(f\"{project_path}test_combined_features.pkl\", \"Test Combined Features\")\n",
    "  print(\"Combined features loaded successfully from checkpoints.\")\n",
    "\n",
    "  if os.path.exists(model_checkpoint) and is_checkpoint_loaded:\n",
    "    # Tải lại checkpoint\n",
    "    clf = joblib.load(model_checkpoint)\n",
    "    print(\"Model checkpoint loaded successfully.\")\n",
    "  else:\n",
    "    # Huấn luyện mô hình Random Forest\n",
    "    print(\"Huấn luyện mô hình Random Forest\")\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced')\n",
    "    clf.fit(X_train, train_labels)\n",
    "    print(\"Model trained successfully.\")\n",
    "    # Lưu mô hình đã huấn luyện\n",
    "    joblib.dump(clf, model_checkpoint)\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "  # Đánh giá mô hình\n",
    "  print(\"KẾT QUẢ ĐÁNH GIÁ MÔ HÌNH RANDOM FOREST\")\n",
    "  y_pred = clf.predict(X_test)\n",
    "  train_accuracy = accuracy_score(train_labels, clf.predict(X_train))\n",
    "  test_accuracy = accuracy_score(test_labels, y_pred)\n",
    "  print(f\"Train Accuracy: {train_accuracy}\")\n",
    "  print(f\"Test Accuracy: {test_accuracy}\")\n",
    "  print(\"Classification Report:\")\n",
    "  print(classification_report(test_labels, y_pred))\n",
    "  print(f\"AUC: {roc_auc_score(test_labels, clf.predict_proba(X_test)[:, 1])}\")\n",
    "  print(f\"F1 Score: {f1_score(test_labels, y_pred)}\")\n",
    "  print(f\"Precision: {precision_score(test_labels, y_pred)}\")\n",
    "  print(f\"Recall: {recall_score(test_labels, y_pred)}\")\n",
    "\n",
    "  # Cross-validation\n",
    "  cv_scores = cross_val_score(clf, X_train, train_labels, cv=5, scoring='accuracy')\n",
    "  print(\"Cross-validation scores:\", cv_scores)\n",
    "  print(\"Mean CV accuracy:\", cv_scores.mean())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "  print(f\"Lỗi xảy ra: {e}. Vui lòng kiểm tra lại đường dẫn file.\")\n",
    "except Exception as e:\n",
    "  print(f\"Lỗi không xác định: {e}. Vui lòng kiểm tra lại dữ liệu hoặc logic code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-KM5OzO-iaA"
   },
   "source": [
    "# PHẦN 5.1: MACHINE LEARNING - THUẬT TOÁN RANDOM FOREST - TỐI ƯU HÓA BẰNG GridSearchCV\n",
    "- Dùng thuật toán RANDOM FOREST - Tối ưu hóa bằng GridSearchCV\n",
    "- Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266658,
     "status": "ok",
     "timestamp": 1734686595870,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "azZCuFKD-sMh",
    "outputId": "d94c3330-aa13-432d-83a6-e5154715d3a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiểm tra và Đảm bảo load dữ liệu từ phần 1\n",
      "Train-Test Data loaded successfully.\n",
      "Train and test data loaded successfully from checkpoints.\n",
      "Kiểm tra và Đọc lại checkpoint đặc trưng của Train và Test Data từ Phần 4\n",
      "Train Combined Features loaded successfully.\n",
      "Test Combined Features loaded successfully.\n",
      "Combined features loaded successfully from checkpoints.\n",
      "Model checkpoint loaded successfully.\n",
      "KẾT QUẢ ĐÁNH GIÁ MÔ HÌNH RANDOM FOREST - tối ưu với GridSearchCV\n",
      "Train Accuracy: 0.9986895604006404\n",
      "Test Accuracy: 0.9977616592055307\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17647\n",
      "           1       1.00      1.00      1.00     17647\n",
      "\n",
      "    accuracy                           1.00     35294\n",
      "   macro avg       1.00      1.00      1.00     35294\n",
      "weighted avg       1.00      1.00      1.00     35294\n",
      "\n",
      "AUC: 0.9999582408271617\n",
      "F1 Score: 0.9977585473116755\n",
      "Precision: 0.9991476304125468\n",
      "Recall: 0.9963733212444041\n",
      "Cross-validation scores: [0.99826457 0.99808748 0.99741456 0.99744997 0.9984416 ]\n",
      "Mean CV accuracy: 0.997931634082488\n"
     ]
    }
   ],
   "source": [
    "#Huấn luyện và đánh giá mô hình\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV # Dùng để xử lý nhanh hơn\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "project_path =\"C:\\\\Users\\\\Innotech_mobile13\\\\Documents\\\\Huit\\\\social_network\\\\ML-RandomForest-backend\\\\models\\\\\"\n",
    "best_model_checkpoint = f\"{project_path}ML_RandomForest_model_with_GridSearchCV.pkl\"\n",
    "is_checkpoint_loaded = True\n",
    "\n",
    "# Hàm load check point\n",
    "def load_checkpoint(path, description=\"Checkpoint\"):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            print(f\"{description} loaded successfully.\")\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{description} not found at {path}.\")\n",
    "\n",
    "try:\n",
    "  # Đường dẫn checkpoint các file gộp đặc trưng của Train và Test Data từ Phần 4\n",
    "  train_combined_checkpoint_path = f\"{project_path}train_combined_features.pkl\"\n",
    "  test_combined_checkpoint_path = f\"{project_path}test_combined_features.pkl\"\n",
    "\n",
    "  # Load dữ liệu Train_data và Test_data từ phần 1\n",
    "  # Đường dẫn file lưu dữ liệu huấn luyện\n",
    "  train_test_data_checkpoint_path = f\"{project_path}train_test_data.pkl\"\n",
    "  print(\"Kiểm tra và Đảm bảo load dữ liệu từ phần 1\")\n",
    "  train_test_data = load_checkpoint(f\"{project_path}train_test_data.pkl\", \"Train-Test Data\")\n",
    "  train_labels, test_labels = train_test_data[\"train_labels\"], train_test_data[\"test_labels\"]\n",
    "  print(\"Train and test data loaded successfully from checkpoints.\")\n",
    "\n",
    "  # Kiểm tra và Đọc lại checkpoint từ Phần 4\n",
    "  print(\"Kiểm tra và Đọc lại checkpoint đặc trưng của Train và Test Data từ Phần 4\")\n",
    "  X_train = load_checkpoint(f\"{project_path}train_combined_features.pkl\", \"Train Combined Features\")\n",
    "  X_test = load_checkpoint(f\"{project_path}test_combined_features.pkl\", \"Test Combined Features\")\n",
    "  print(\"Combined features loaded successfully from checkpoints.\")\n",
    "\n",
    "  # Khởi tạo tham số cho GridSearch\n",
    "  param_grid = {\n",
    "      'n_estimators': [50, 100, 200],\n",
    "      'max_depth': [None, 10, 20, 30],\n",
    "      'min_samples_split': [2, 5, 10],\n",
    "      'min_samples_leaf': [1, 2, 4],\n",
    "      'class_weight': ['balanced', 'balanced_subsample']\n",
    "  }\n",
    "\n",
    "  if os.path.exists(best_model_checkpoint) and is_checkpoint_loaded:\n",
    "    # Tải lại checkpoint\n",
    "    best_model = joblib.load(best_model_checkpoint)\n",
    "    print(\"Model checkpoint loaded successfully.\")\n",
    "  else:\n",
    "    # Huấn luyện mô hình Random Forest\n",
    "    print(\"Huấn luyện mô hình Random Forest - Tối ưu hóa với GridSearchCV\")\n",
    "    # Khởi tạo mô hình cơ bản\n",
    "    base_model = RandomForestClassifier(random_state=42)\n",
    "    # GridSearchCV để tối ưu tham số\n",
    "    #grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    # Chuyển qua RandomizedSearchCV để có thể vừa đạt kết quả tốt vừa tiết kiệm thời gian\n",
    "    grid_search = RandomizedSearchCV(\n",
    "                                estimator=RandomForestClassifier(random_state=42),\n",
    "                                param_distributions=param_grid,\n",
    "                                n_iter=20,  # Số lượng tổ hợp ngẫu nhiên\n",
    "                                cv=5,\n",
    "                                scoring='accuracy',\n",
    "                                verbose=1,\n",
    "                                n_jobs=-1,\n",
    "                                random_state=42\n",
    "                            )\n",
    "\n",
    "    print(\"Bắt đầu tìm kiếm tham số tối ưu...\")\n",
    "    grid_search.fit(X_train, train_labels)  # Huấn luyện trên tập train\n",
    "\n",
    "    # Tìm ra tham số tốt nhất và mô hình tối ưu\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best cross-validated accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    print(\"Model trained successfully.\")\n",
    "    # Lưu mô hình đã huấn luyện\n",
    "    joblib.dump(best_model, best_model_checkpoint)\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "  # Đánh giá mô hình\n",
    "  print(\"KẾT QUẢ ĐÁNH GIÁ MÔ HÌNH RANDOM FOREST - tối ưu với GridSearchCV\")\n",
    "  y_pred = best_model.predict(X_test)\n",
    "  train_accuracy = accuracy_score(train_labels, best_model.predict(X_train))\n",
    "  test_accuracy = accuracy_score(test_labels, y_pred)\n",
    "  print(f\"Train Accuracy: {train_accuracy}\")\n",
    "  print(f\"Test Accuracy: {test_accuracy}\")\n",
    "  print(\"Classification Report:\")\n",
    "  print(classification_report(test_labels, y_pred))\n",
    "  print(f\"AUC: {roc_auc_score(test_labels, best_model.predict_proba(X_test)[:, 1])}\")\n",
    "  print(f\"F1 Score: {f1_score(test_labels, y_pred)}\")\n",
    "  print(f\"Precision: {precision_score(test_labels, y_pred)}\")\n",
    "  print(f\"Recall: {recall_score(test_labels, y_pred)}\")\n",
    "\n",
    "  # Cross-validation\n",
    "  cv_scores = cross_val_score(best_model, X_train, train_labels, cv=5, scoring='accuracy')\n",
    "  print(\"Cross-validation scores:\", cv_scores)\n",
    "  print(\"Mean CV accuracy:\", cv_scores.mean())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "  print(f\"Lỗi xảy ra: {e}. Vui lòng kiểm tra lại đường dẫn file.\")\n",
    "except Exception as e:\n",
    "  print(f\"Lỗi không xác định: {e}. Vui lòng kiểm tra lại dữ liệu hoặc logic code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20898,
     "status": "ok",
     "timestamp": 1734769431320,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "kJY2Z6nhrAZS",
    "outputId": "5cc7f837-3219-4f97-d785-9a0f07bc5882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EBWov3t0k9U"
   },
   "source": [
    "PHẦN 6: GIẢ LẬP: RECOMMENDATION - DỰ ĐOÁN CÁC LIÊN KẾT TIỀM NĂNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhXhBdM2D909"
   },
   "source": [
    "Phần 6.1: Tạo danh sách các cạnh chưa có liên kết (potential links) - Tạo 1000 cạnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2825,
     "status": "ok",
     "timestamp": 1734614276980,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "z_nONUNQEIso",
    "outputId": "5571ca60-eb1e-42ff-ce95-031e10bbed62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo và lưu 1000 potential links tại: /content/drive/MyDrive/Chương trình thạc sĩ/Mạng xã hội - Social Network/Đồ án môn học/potential_links.csv\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn tệp lưu potential_links\n",
    "project_path = \"/content/drive/MyDrive/Chương trình thạc sĩ/Mạng xã hội - Social Network/Đồ án môn học/\"\n",
    "processed_graph_path = f\"{project_path}processed_graph.graphml\"\n",
    "potential_links_path = f\"{project_path}potential_links.csv\"\n",
    "\n",
    "# Tải đồ thị đã xử lý từ file processed_graph.graphml\n",
    "G = nx.read_graphml(processed_graph_path)\n",
    "\n",
    "# Số lượng potential links cần tạo\n",
    "num_links = 1000\n",
    "\n",
    "# Tạo danh sách các node\n",
    "nodes = list(G.nodes())\n",
    "existing_edges = set(G.edges())\n",
    "potential_links = set()\n",
    "\n",
    "# Tạo các cạnh tiềm năng\n",
    "while len(potential_links) < num_links:\n",
    "    u, v = random.sample(nodes, 2)  # Chọn 2 node ngẫu nhiên\n",
    "    if (u, v) not in existing_edges and (v, u) not in existing_edges and (u, v) not in potential_links:\n",
    "        potential_links.add((u, v))\n",
    "\n",
    "# Chuyển đổi sang DataFrame và lưu thành file CSV\n",
    "potential_links_df = pd.DataFrame(list(potential_links), columns=[\"node1\", \"node2\"])\n",
    "potential_links_df.to_csv(potential_links_path, index=False)\n",
    "\n",
    "print(f\"Đã tạo và lưu {num_links} potential links tại: {potential_links_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o5BFat-EkBP"
   },
   "source": [
    "Phần 6.2: Dự đoán liên kết tiềm năng từ danh sách potential links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1734614671897,
     "user": {
      "displayName": "Quang Nguyen",
      "userId": "06976087321846366882"
     },
     "user_tz": -420
    },
    "id": "cagGI64Zp5Ip",
    "outputId": "d4bc9457-786b-426b-807b-bb306f6ef6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu kiểm thử Recommendation...\n",
      "Random Forest model loaded successfully.\n",
      "Node2Vec model loaded successfully.\n",
      "Nodes features loaded successfully!\n",
      "Loaded 1000 potential links.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing features for potential links: 100%|██████████| 1000/1000 [00:00<00:00, 166698.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danh sách top 20 liên kết được dự đoán dựa trên xác suất cao nhất\n",
      "            link  probability\n",
      "0   [2479, 3962]          1.0\n",
      "1   [2565, 1998]          1.0\n",
      "2   [3453, 3073]          1.0\n",
      "3    [370, 1253]          1.0\n",
      "4   [1222, 1068]          1.0\n",
      "5     [3848, 22]          1.0\n",
      "6   [1630, 3978]          1.0\n",
      "7    [954, 2762]          1.0\n",
      "8   [1397, 2442]          1.0\n",
      "9   [3458, 3477]          1.0\n",
      "10   [1136, 226]          1.0\n",
      "11   [2615, 165]          1.0\n",
      "12  [1889, 3878]          1.0\n",
      "13  [1495, 1836]          1.0\n",
      "14  [2517, 1258]          1.0\n",
      "15  [3164, 2163]          1.0\n",
      "16   [2409, 158]          1.0\n",
      "17  [3032, 3831]          1.0\n",
      "18  [3131, 2520]          1.0\n",
      "19  [2445, 1096]          1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Đường dẫn\n",
    "model_checkpoint = f\"C:\\\\Users\\\\Innotech_mobile13\\\\Documents\\\\Huit\\\\social_network\\\\ML-RandomForest-backend\\\\models\\\\ML_RandomForest_model_with_GridSearchCV.pkl\"\n",
    "node2vec_full_model_checkpoint_path = f\"C:\\\\Users\\\\Innotech_mobile13\\\\Documents\\\\Huit\\\\social_network\\\\ML-RandomForest-backend\\\\models\\\\node2vec_full_model.pkl\"\n",
    "\n",
    "# Đường dẫn checkpoint đặc trưng\n",
    "nodes_features_checkpoint_path = f\"C:\\\\Users\\\\Innotech_mobile13\\\\Documents\\\\Huit\\\\social_network\\\\ML-RandomForest-backend\\\\models\\\\nodes_features.pkl\"\n",
    "\n",
    "# Đọc danh sách potential_links (cần tạo sẵn trước)\n",
    "potential_links_path = f\"C:\\\\Users\\\\Innotech_mobile13\\\\Documents\\\\Huit\\\\social_network\\\\ML-RandomForest-backend\\\\models\\\\potential_links.csv\"  # Danh sách cạnh tiềm năng\n",
    "#output_recommendation_path = f\"{project_path}recommendation_links.csv\"\n",
    "\n",
    "# Hàm chuẩn bị đặc trưng cho các cạnh tiềm năng\n",
    "def prepare_features(data, feature_dict, model):\n",
    "    combined_features = []\n",
    "    for u, v in tqdm(data, desc=\"Preparing features for potential links\"):\n",
    "        if u in model.wv and v in model.wv:\n",
    "            # Vector nhúng của u và v\n",
    "            embedding_u = model.wv[u]\n",
    "            embedding_v = model.wv[v]\n",
    "\n",
    "            # Đặc trưng thủ công\n",
    "            handcrafted_features = feature_dict.get((u, v), np.zeros(7))  # Nếu không có đặc trưng, thay bằng 0\n",
    "\n",
    "            # Kết hợp vector nhúng và đặc trưng thủ công\n",
    "            combined_features.append(\n",
    "                np.concatenate([embedding_u, embedding_v, handcrafted_features])\n",
    "            )\n",
    "        else:\n",
    "            combined_features.append(None)  # Lưu None nếu không có vector nhúng\n",
    "    return combined_features\n",
    "\n",
    "try:\n",
    "    print(\"Bắt đầu kiểm thử Recommendation...\")\n",
    "\n",
    "    # Tải mô hình đã huấn luyện\n",
    "    clf = joblib.load(model_checkpoint)\n",
    "    print(\"Random Forest model loaded successfully.\")\n",
    "\n",
    "    # Tải Node2Vec model\n",
    "    with open(node2vec_full_model_checkpoint_path, 'rb') as f:\n",
    "        node2vec_model = pickle.load(f)\n",
    "    print(\"Node2Vec model loaded successfully.\")\n",
    "\n",
    "    # Tải đặc trưng từ checkpoints\n",
    "    try:\n",
    "        if os.path.exists(nodes_features_checkpoint_path):\n",
    "            with open(nodes_features_checkpoint_path, 'rb') as f:\n",
    "                features_data = pickle.load(f)\n",
    "            nodes_features = features_data  # Cập nhật biến nodes_features\n",
    "            print(\"Nodes features loaded successfully!\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File nodes_features.pkl không tồn tại!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải nodes_features: {e}\")\n",
    "\n",
    "    # Tải danh sách potential links\n",
    "    potential_links = pd.read_csv(potential_links_path).values.tolist()\n",
    "    print(f\"Loaded {len(potential_links)} potential links.\")\n",
    "\n",
    "    # Chuẩn bị đặc trưng cho potential links\n",
    "    potential_features = prepare_features(potential_links, nodes_features, node2vec_model)\n",
    "\n",
    "    # Loại bỏ các cạnh không hợp lệ (None features)\n",
    "    valid_links = [link for link, features in zip(potential_links, potential_features) if features is not None]\n",
    "    valid_features = [features for features in potential_features if features is not None]\n",
    "\n",
    "    # Dự đoán xác suất\n",
    "    predictions = clf.predict_proba(valid_features)[:, 1]  # Xác suất của lớp 1 (có cạnh)\n",
    "\n",
    "    # Sắp xếp và chọn top 20 recommendation links\n",
    "    top_indices = np.argsort(predictions)[-20:][::-1]  # Lấy 20 giá trị cao nhất\n",
    "    top_links = [(valid_links[i], predictions[i]) for i in top_indices]\n",
    "\n",
    "    recommendation_df = pd.DataFrame(top_links, columns=[\"link\", \"probability\"])\n",
    "\n",
    "    # Xuất kết quả ra file CSV\n",
    "    #recommendation_df.to_csv(output_recommendation_path, index=False)\n",
    "    #print(f\"Đã hoàn thành kiểm thử Recommendation. Kết quả được lưu tại: {output_recommendation_path}\")\n",
    "\n",
    "    print(f\"Danh sách top 20 liên kết được dự đoán dựa trên xác suất cao nhất\")\n",
    "    print(recommendation_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi xảy ra khi kiểm thử Recommendation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2KBhGqYHou1"
   },
   "source": [
    "Phần 6.3: Giải lập - Tìm 10 potential links (xác suất cao nhất) khi đưa 1 node vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model loaded successfully.\n",
      "Node2Vec model loaded successfully.\n",
      "Nodes features loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing features for potential links: 100%|██████████| 4022/4022 [00:00<00:00, 201104.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for node 24:\n",
      "1. Link: ('24', '2069') - Score: 0.9995\n",
      "2. Link: ('24', '2266') - Score: 0.9995\n",
      "3. Link: ('24', '2034') - Score: 0.9995\n",
      "4. Link: ('24', '629') - Score: 0.9995\n",
      "5. Link: ('24', '650') - Score: 0.9995\n",
      "6. Link: ('24', '2044') - Score: 0.9995\n",
      "7. Link: ('24', '2058') - Score: 0.9995\n",
      "8. Link: ('24', '1105') - Score: 0.9995\n",
      "9. Link: ('24', '364') - Score: 0.9995\n",
      "10. Link: ('24', '2312') - Score: 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hàm tạo đặc trưng cho các liên kết tiềm năng (potential links)\n",
    "def generate_features_for_potential_links(data, feature_dict, model):\n",
    "    combined_features = []\n",
    "    for u, v in tqdm(data, desc=\"Preparing features for potential links\"):\n",
    "        if u in model.wv and v in model.wv:\n",
    "            # Vector nhúng của u và v\n",
    "            embedding_u = model.wv[u]\n",
    "            embedding_v = model.wv[v]\n",
    "\n",
    "            # Đặc trưng thủ công\n",
    "            handcrafted_features = feature_dict.get((u, v), np.zeros(7))  # Nếu không có đặc trưng, thay bằng 0\n",
    "\n",
    "            # Kết hợp vector nhúng và đặc trưng thủ công\n",
    "            combined_features.append(\n",
    "                np.concatenate([embedding_u, embedding_v, handcrafted_features])\n",
    "            )\n",
    "        else:\n",
    "            combined_features.append(None)  # Lưu None nếu không có vector nhúng\n",
    "    return combined_features\n",
    "\n",
    "# Hàm gợi ý các liên kết tiềm năng cho một node\n",
    "def recommend_links_for_node(input_node, graph, model, node2vec_model, nodes_features, top_k=10):\n",
    "    # Lấy tất cả các node trong đồ thị\n",
    "    all_nodes = list(graph.nodes())\n",
    "\n",
    "    # Tạo danh sách các cặp node tiềm năng (u, v) với u là input_node và v là tất cả các node khác\n",
    "    potential_links = [(input_node, v) for v in all_nodes if v != input_node and not graph.has_edge(input_node, v)]\n",
    "\n",
    "    # Tạo đặc trưng cho các potential links\n",
    "    X_potential_links = generate_features_for_potential_links(potential_links, nodes_features, node2vec_model)\n",
    "\n",
    "    # Dự đoán xác suất cho các liên kết tiềm năng\n",
    "    predicted_probs = model.predict_proba(X_potential_links)[:, 1]  # Lấy xác suất của lớp 1 (có liên kết)\n",
    "\n",
    "    # Gợi ý top_k liên kết có xác suất cao nhất\n",
    "    top_k_indices = np.argsort(predicted_probs)[-top_k:][::-1]\n",
    "    top_k_recommendations = [potential_links[i] for i in top_k_indices]\n",
    "    top_k_scores = [predicted_probs[i] for i in top_k_indices]\n",
    "\n",
    "    return top_k_recommendations, top_k_scores\n",
    "\n",
    "# Hàm chính để kiểm thử\n",
    "def test_recommendation(input_node, graph, model, node2vec_model, nodes_features, top_k=10):\n",
    "    try:\n",
    "        # Gọi hàm gợi ý để nhận các liên kết tiềm năng\n",
    "        recommendations, scores = recommend_links_for_node(input_node, graph, model, node2vec_model, nodes_features, top_k)\n",
    "\n",
    "        print(f\"Top {top_k} recommendations for node {input_node}:\")\n",
    "        for i, (link, score) in enumerate(zip(recommendations, scores)):\n",
    "            print(f\"{i+1}. Link: {link} - Score: {score:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during recommendation: {e}\")\n",
    "\n",
    "# Đường dẫn\n",
    "project_path = f\"models\\\\\"\n",
    "processed_graph_path = f\"{project_path}processed_graph.graphml\"\n",
    "model_checkpoint = f\"{project_path}ML_RandomForest_model.pkl\"\n",
    "node2vec_full_model_checkpoint_path = f\"{project_path}node2vec_full_model.pkl\"\n",
    "# Đường dẫn checkpoint đặc trưng\n",
    "nodes_features_checkpoint_path = f\"{project_path}nodes_features.pkl\"\n",
    "\n",
    "# Tải đồ thị đã xử lý từ file processed_graph.graphml\n",
    "G = nx.read_graphml(processed_graph_path)\n",
    "# Tải mô hình đã huấn luyện\n",
    "clf = joblib.load(model_checkpoint)\n",
    "print(\"Random Forest model loaded successfully.\")\n",
    "# Tải Node2Vec model\n",
    "with open(node2vec_full_model_checkpoint_path, 'rb') as f:\n",
    "    node2vec_model = pickle.load(f)\n",
    "    print(\"Node2Vec model loaded successfully.\")\n",
    "\n",
    "# Tải đặc trưng từ checkpoints\n",
    "try:\n",
    "    if os.path.exists(nodes_features_checkpoint_path):\n",
    "        with open(nodes_features_checkpoint_path, 'rb') as f:\n",
    "            features_data = pickle.load(f)\n",
    "        nodes_features = features_data  # Cập nhật biến nodes_features\n",
    "        print(\"Nodes features loaded successfully!\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"File nodes_features.pkl không tồn tại!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tải nodes_features: {e}\")\n",
    "\n",
    "# Ví dụ về cách sử dụng hàm trên\n",
    "input_node = \"24\"  # Chọn một node bất kỳ\n",
    "top_k = 10  # Số lượng liên kết tiềm năng cần gợi ý\n",
    "\n",
    "# Kiểm thử với model và dữ liệu đã được huấn luyện và load sẵn\n",
    "test_recommendation(input_node, G, clf, node2vec_model, nodes_features, top_k)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "181e7b942f2245d2b52dcb9a51cd46c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ec1525d2f174217a4fe503fedfdc815": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32ba96ddee12409783431c2ecc0b44e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9837c2b906ad41c4aff8b111d9cdfa21",
      "max": 4039,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33592f9cb1654d9ea83b33c2079c423b",
      "value": 4039
     }
    },
    "33592f9cb1654d9ea83b33c2079c423b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "360dc743c11447c88fccbb159d26a634": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a178cc36a51456f89a6d7bf79767196": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62866b75178a4fde848a3ed7833dad40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ec1525d2f174217a4fe503fedfdc815",
      "placeholder": "​",
      "style": "IPY_MODEL_ed9e8777566843eb8186aded8ffbd609",
      "value": "Computing transition probabilities: 100%"
     }
    },
    "664fa514bb1543a398e3a95380773ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a178cc36a51456f89a6d7bf79767196",
      "placeholder": "​",
      "style": "IPY_MODEL_181e7b942f2245d2b52dcb9a51cd46c8",
      "value": " 4039/4039 [01:21&lt;00:00, 182.38it/s]"
     }
    },
    "9837c2b906ad41c4aff8b111d9cdfa21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9953e264b12046479b43b8e3269be48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62866b75178a4fde848a3ed7833dad40",
       "IPY_MODEL_32ba96ddee12409783431c2ecc0b44e1",
       "IPY_MODEL_664fa514bb1543a398e3a95380773ed6"
      ],
      "layout": "IPY_MODEL_360dc743c11447c88fccbb159d26a634"
     }
    },
    "ed9e8777566843eb8186aded8ffbd609": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
